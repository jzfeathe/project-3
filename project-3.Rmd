---
title: "project-3"
author: "Justin Feathers"
date: '2022-11-01'
output: 
  github_document:
    toc: true
    toc_depth: 2
    df_print: paged
params:
    channel: "tech"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
```

# Introduction
The dataset we'll be exploring is a collection of online news articles published by
Mashable in a period of two years. By analyzing a subset of the variables from the 
dataset, we hope to predict the response variable, `shares`. Using correlation plots,
I have decided to use `num_hrefs` (number of links in the article), 
`n_tokens_content` (number of words in the article), `kw_avg_avg` (mean shares for
the average number of keywords), `n_non_stop_unique_tokens` (the rate of unique
non-stop words in the content), `num_videos` (the number of videos), 
`min_negative_polarity` (minimum polarity of negative words), `rate_positive_words`
(rate of positive words among non-neutral tokens), and `min_positive_polarity` 
(minimum polarity of positive words) as our predictive variables. I will go into
more detail with the variable selection process later.

We will explore
these variables through a number of summary statistics and plots and finally
use multiple linear regression and random forest models to make predictions for
the `shares` variable.

# Data
The first step in any analysis is to call the required packages and read in the 
dataset. Here, we use the `get` function to tell R to use the character string
as a variable object -- in this case, the appropriate `parms` value. By using a 
logical statement we are able to choose only the rows corresponding to our news
channel. Because `url` and `timedelta` are not predictive variables, they have 
been dropped from the dataset.
```{r}
library(tidyverse)
library(corrplot)
library(caret)
library(shiny)
newsData <- read_csv(file = "./OnlineNewsPopularity.csv")
data <- newsData %>% 
            filter(get(paste0("data_channel_is_", params$channel)) == 1) %>%
              select(-url, -timedelta)
```

# Summarizations
Once the dataset is read in, a good way to start is by checking how strongly 
all variables are correlated to the response variable of interest. This is the 
beginning of the variable selection process. Since we are interested in predicting `shares`,
I created a correlation matrix using the `cor` function and sorted the absolute values
of the output to get a convenient tibble of descending correlation values; this was
originally done using the `tech` data channel to set the ground work. I decided
to choose the 9 variables with the highest correlation (positive or negative) with
`shares`.

From here, we can look at a correlation plot of the chosen variables to see if 
multicollinearity exists between any of them. Using `corrplot`, we can 
check the relationships (correlation) between the chosen variables. Though subjective
in nature, a general rule of thumb is that any correlation greater than 0.70 or 
less than -0.70 between variables (not involving the response) is considered a 
problematic level of multicollinearity. In the original assessment, `n_unique_tokens`
had a high correlation with 2 different variables, so it was dropped from the dataset.

Next, we should look at some summary data with `summary()` to get a feel for the ranges
of the predictors and response.
```{r}
dataCor <- cor(data$shares, data) %>%
        as.tibble() %>%
        abs() %>%
        sort(decreasing = TRUE)
dataCor

data <- data %>% 
            select(num_hrefs, n_tokens_content, n_unique_tokens, kw_avg_avg,
                   n_non_stop_unique_tokens, num_videos, min_negative_polarity,
                   rate_positive_words, min_positive_polarity, shares)

correlation <- cor(data)
corrplot(correlation, type = "upper", tl.pos = "lt")
corrplot(correlation, type = "lower", method = "number",
         add = TRUE, diag = FALSE, tl.pos = "n")

data <- data %>%
            select(-n_unique_tokens)

correlation <- cor(data)
corrplot(correlation, type = "upper", tl.pos = "lt")
corrplot(correlation, type = "lower", method = "number",
         add = TRUE, diag = FALSE, tl.pos = "n")

summary(data)
mean <- mean(data$shares)
sd <- sd(data$shares)
g <- ggplot(data, aes(x = shares))
g + geom_histogram()
interval <- 3*sd(data$shares) + mean(data$shares)
```

We can see from the summary statistics and histogram that `shares` has a very 
large max value, but a small mean and q3, comparatively. This indicates that the
distribution is strongly skewed left. We can fix this by temporarily removing
outliers to make data visualization more meaningful. Using the mean and standard
deviation for `shares`, we can calculate an interval that gives us 3 standard deviations
above the mean, which means we will encompass 99.7% of the data while removing outliers.
Because we can't have negative shares, we will have values between 0 and `r interval`. 

First, we'll take a look at a plot without the adjustment for comparison's sake.
Clearly, the adjustment does, indeed, make it easier to observe trends in the data.
Now we can inspect some scatter plots with our improved `shares` variable.  
```{r}
g <- ggplot(data, aes(y = shares))
g + geom_point(aes(x = num_hrefs))
```
Here is a scatter plot of `num_hrefs` by `shares`. A positive trend would indicate
that shares should increase as the number of links in the article increases. If
we observe a negative trend, the number of shares should decrease as the number
of links increases.
```{r}
temp <- data %>%
          filter(shares < interval)
g <- ggplot(temp, aes(y = shares))
g + geom_point(aes(x = num_hrefs))
```

Next is a scatter plot of `n_tokens_content` by `shares`. A positive trend would indicate
that shares should increase as the number of words in the article increases; likewise,
the number of shares should decrease as word count increases if we observe a negative
trend.
```{r}
g + geom_point(aes(x = n_tokens_content))
```

And finally, we have a scatter plot of `rate_positive_words` by `shares`. Again,
a positive trend would indicate that we should observe an increase in shares as 
as the proportion of positive words to non-neutral words increases while a 
negative trend would indicate a decrease in shares as the proportion of positive 
words increases to non-neutral words increases.
```{r}
g + geom_point(aes(x = rate_positive_words))
```

# Modeling
```{r}
set.seed(250)
index <- createDataPartition(data$shares, p = 0.70, list = FALSE)
train <- data[index, ]
test <- data[-index, ]
```

## Multiple Linear Regression
Fitting a multiple regression model on all variables in the `data` dataset,
we can see from the `summary` function that this model is not a very good fit with
an adjusted R^2 value of 0.01 -- this means only 1% of the variance in the data is
explained by the model. We need to explore better options.

```{r}
mlrFit <- train(shares ~ ., data = train,
                preProcess = c("center", "scale"),
                method = "lm",
                trControl = trainControl(method = "cv", number = 5))
mlrPredict <- predict(mlrFit, newdata = test)
temp <- postResample(mlrPredict, test$shares)
mlrRsquare <- temp[2]
mlrRsquare
```

## Random Forest
```{r}
forest <- train(shares ~ ., data = train,
                method = "rf",
                preProcess = c("center", "scale"),
                tuneGrid = data.frame(mtry = ncol(train)/3),
                trControl = trainControl(method = "cv", number = 5))
forestPredict <- predict(forest, newdata = test)
temp <- postResample(forestPredict, test$shares)
forestRsquare <- temp[2]
forestRsquare
```

# Comparison
To compare the models, we will use a simple comparison of R^2 and choose the
one with the highest value. We will use this method since R^2 can be interpreted
as how much of the variance in the data can be explained by the model, i.e., how
well the model fits.
```{r}
if (mlrRsquare > forestRsquare) {
  paste0("Multiple linear regression is the preferred model for data channel = ", params$channel)
} else {
  paste0("Random forest is the preferred model for data channel = ", params$channel)
} 
```
